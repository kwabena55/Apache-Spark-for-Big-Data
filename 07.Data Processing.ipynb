{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "8fac594bfae6525c0c41b4041d2d72effa188cc8ead05f81b1fab2bb098927fb"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Read data File and Create Data Schema"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "spark=SparkSession.builder.appName(\"Data Processing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv(\"C:/Users/User/Desktop/SparkFolder/Data/raw-flight-data.csv\",inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+\n|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n+----------+---------+-------+---------------+-------------+--------+--------+\n|        19|        5|     DL|          11433|        13303|      -3|       1|\n|        19|        5|     DL|          14869|        12478|       0|      -8|\n|        19|        5|     DL|          14057|        14869|      -4|     -15|\n|        19|        5|     DL|          15016|        11433|      28|      24|\n|        19|        5|     DL|          11193|        12892|      -6|     -11|\n|        19|        5|     DL|          10397|        15016|      -1|     -19|\n|        19|        5|     DL|          15016|        10397|       0|      -1|\n|        19|        5|     DL|          10397|        14869|      15|      24|\n|        19|        5|     DL|          10397|        10423|      33|      34|\n|        19|        5|     DL|          11278|        10397|     323|     322|\n|        19|        5|     DL|          14107|        13487|      -7|     -13|\n|        19|        5|     DL|          11433|        11298|      22|      41|\n|        19|        5|     DL|          11298|        11433|      40|      20|\n|        19|        5|     DL|          11433|        12892|      -2|      -7|\n|        19|        5|     DL|          10397|        12451|      71|      75|\n|        19|        5|     DL|          12451|        10397|      75|      57|\n|        19|        5|     DL|          12953|        10397|      -1|      10|\n|        19|        5|     DL|          11433|        12953|      -3|     -10|\n|        19|        5|     DL|          10397|        14771|      31|      38|\n|        19|        5|     DL|          13204|        10397|       8|      25|\n+----------+---------+-------+---------------+-------------+--------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- DayofMonth: integer (nullable = true)\n |-- DayOfWeek: integer (nullable = true)\n |-- Carrier: string (nullable = true)\n |-- OriginAirportID: integer (nullable = true)\n |-- DestAirportID: integer (nullable = true)\n |-- DepDelay: integer (nullable = true)\n |-- ArrDelay: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "source": [
    "Redefine Schema for Flights"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "flightschema=StructType([\n",
    "\n",
    "                            StructField(\"DayofMonth\",IntegerType(), False),\n",
    "                            StructField(\"DayOfWeek\",IntegerType(), False),\n",
    "                            StructField(\"Carrier\",StringType(), False),\n",
    "                            StructField(\"OriginAirportID\",IntegerType(), False),\n",
    "                            StructField(\"DestAirportID\",IntegerType(), False),\n",
    "                            StructField(\"DepDelay\",IntegerType(), False),\n",
    "                            StructField(\"ArrDelay\",IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv(\"C:/Users/User/Desktop/SparkFolder/Data/raw-flight-data.csv\",schema=flightschema,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+\n|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n+----------+---------+-------+---------------+-------------+--------+--------+\n|        19|        5|     DL|          11433|        13303|      -3|       1|\n|        19|        5|     DL|          14869|        12478|       0|      -8|\n|        19|        5|     DL|          14057|        14869|      -4|     -15|\n|        19|        5|     DL|          15016|        11433|      28|      24|\n|        19|        5|     DL|          11193|        12892|      -6|     -11|\n|        19|        5|     DL|          10397|        15016|      -1|     -19|\n|        19|        5|     DL|          15016|        10397|       0|      -1|\n|        19|        5|     DL|          10397|        14869|      15|      24|\n|        19|        5|     DL|          10397|        10423|      33|      34|\n|        19|        5|     DL|          11278|        10397|     323|     322|\n|        19|        5|     DL|          14107|        13487|      -7|     -13|\n|        19|        5|     DL|          11433|        11298|      22|      41|\n|        19|        5|     DL|          11298|        11433|      40|      20|\n|        19|        5|     DL|          11433|        12892|      -2|      -7|\n|        19|        5|     DL|          10397|        12451|      71|      75|\n|        19|        5|     DL|          12451|        10397|      75|      57|\n|        19|        5|     DL|          12953|        10397|      -1|      10|\n|        19|        5|     DL|          11433|        12953|      -3|     -10|\n|        19|        5|     DL|          10397|        14771|      31|      38|\n|        19|        5|     DL|          13204|        10397|       8|      25|\n+----------+---------+-------+---------------+-------------+--------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "source": [
    "Load the Airport Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=spark.read.csv(\"C:/Users/User/Desktop/SparkFolder/Data/airports.csv\",inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-----------+-----+--------------------+\n|airport_id|       city|state|                name|\n+----------+-----------+-----+--------------------+\n|     10165|Adak Island|   AK|                Adak|\n|     10299|  Anchorage|   AK|Ted Stevens Ancho...|\n|     10304|      Aniak|   AK|       Aniak Airport|\n|     10754|     Barrow|   AK|Wiley Post/Will R...|\n|     10551|     Bethel|   AK|      Bethel Airport|\n+----------+-----------+-----+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- airport_id: integer (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "source": [
    "Redefine Schema for Airports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportschema=StructType([\n",
    "\n",
    "                            StructField(\"airport_id\",IntegerType(), False),\n",
    "                            StructField(\"city\",StringType(), False),\n",
    "                            StructField(\"state\",StringType(), False),\n",
    "                            StructField(\"name\",StringType(), False)\n",
    "                            \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=spark.read.csv(\"C:/Users/User/Desktop/SparkFolder/Data/airports.csv\",schema=airportschema,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- airport_id: integer (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-----------+-----+--------------------+\n|airport_id|       city|state|                name|\n+----------+-----------+-----+--------------------+\n|     10165|Adak Island|   AK|                Adak|\n|     10299|  Anchorage|   AK|Ted Stevens Ancho...|\n|     10304|      Aniak|   AK|       Aniak Airport|\n|     10754|     Barrow|   AK|Wiley Post/Will R...|\n|     10551|     Bethel|   AK|      Bethel Airport|\n+----------+-----------+-----+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "source": [
    "Merge the Two Data to show the flights from each city"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightbyOrigin=df1.join(df,df[\"OriginAirportID\"]==df1[\"airport_id\"]).groupBy(\"city\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------+------+\n|             city| count|\n+-----------------+------+\n|          Phoenix| 90281|\n|            Omaha| 13537|\n|   Raleigh/Durham| 28436|\n|        Anchorage|  7777|\n|           Dallas| 19503|\n|          Oakland| 25503|\n|      San Antonio| 23090|\n|     Philadelphia| 47659|\n|       Louisville| 10953|\n|Dallas/Fort Worth|105024|\n|      Los Angeles|118684|\n|       Sacramento| 25193|\n|     Indianapolis| 18099|\n|        Cleveland| 25261|\n|        San Diego| 45783|\n|    San Francisco| 84675|\n|        Nashville| 34927|\n|    Oklahoma City| 13967|\n|          Detroit| 62879|\n|         Portland| 30640|\n+-----------------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "flightbyOrigin.show()"
   ]
  },
  {
   "source": [
    "Handling Duplicated Data\n",
    "* Drop duplicated and calculate duplicated Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of rows is : 2719418\n",
      "The number of duplicates dropped : 2696983\n",
      "The number of duplicated data : 22435\n"
     ]
    }
   ],
   "source": [
    "#Total number of rows\n",
    "n1=df.count()\n",
    "print(\"The number of rows is :\",n1)\n",
    "#Drop Duplicates\n",
    "n2=df.dropDuplicates().count()\n",
    "print(\"The number of duplicates dropped :\",n2)\n",
    "n3=n1-n2\n",
    "print(\"The number of duplicated data :\", n3)"
   ]
  },
  {
   "source": [
    "Create DataFrame from Tuples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df2=spark.createDataFrame([\n",
    "                        (\"Sol\", 54,58),\n",
    "                        (\"Sol\", 57,55),\n",
    "                        (\"Sol\", 54,58)],\n",
    "                        [\"Name\",\"age\",\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2=df2.withColumn(\"ID\",monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+---+------+\n",
      "|Name|age|height|\n",
      "+----+---+------+\n",
      "| Sol| 54|    58|\n",
      "| Sol| 57|    55|\n",
      "| Sol| 54|    58|\n",
      "+----+---+------+\n",
      "\n",
      "+----+---+------+\n",
      "|Name|age|height|\n",
      "+----+---+------+\n",
      "| Sol| 54|    58|\n",
      "| Sol| 57|    55|\n",
      "+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()\n",
    "df2.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+---+------+\n|Name|age|height|\n+----+---+------+\n| Sol| 54|    58|\n+----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.dropDuplicates([\"Name\"]).show()"
   ]
  },
  {
   "source": [
    "Handling Missing Data\n",
    "* Delete row iff there is at least one ( column) missing Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}