{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import image module \n",
    "from IPython.display import Image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whats Spark \n",
    "* Distributed computig engine for processing big data\n",
    "* Similar to Hadoop but differs in in-memory processing\n",
    "* In memory\n",
    "* Hadoop adopts In- disk processing\n",
    "* Spark is 100x faster in memory and 10x faster in disc\n",
    "* Well designed layered achitecture which follow master/slave concept and between master and slave we have the Cluster manager layer\n",
    "* All these 3 are designed well within its boundary layer and loosely coupled to one another\n",
    "* Nodes ---> Each node have N Executors and within every Executors we have N cores \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image]('image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Image(url=\"image.png\", width=300, height=300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4 Exceutors , 64 core  and 128gb Memory \n",
    "Executor 1--> RAM 32gb , Core 16 (A core processing a data becomes a TASKS )\n",
    "Executor 2--> RAM 32gb, Core 16\n",
    "Executor 3--> RAM 32gb, Core 16\n",
    "Executor 4--> RAM 32gb, Core 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Life Cycle of Spark Application\n",
    "*  Use submits a submits application ( Meaning user submits a command or piece of code)\n",
    "* Driver initiates a SparkSession\n",
    "* Directed Acrylic Graph ( DAG) creates a logical plan for all the transformation and will keep a lineage graph for the logical plan to be created.Its acrylic because the its stepwise and not cyclical\n",
    "* DAG will then break the code into multiple tasksand further submitted to task executor which then request resource from the cluster manager\n",
    "* Cluster mananger will then allocate resources to execute the taks\n",
    "*A connection is then established between the driver and the worker directly\n",
    "* Worker executes the tasks and return the results to the driver\n",
    "* Driver returns the results to the user in the UI\n",
    "*Application comes to an **end**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Attributes\n",
    "* Scalability\n",
    "* Fault Tolerant --> Follows a lazy evaluation model which means when we submit a transformation data is not processed immediately but instead logical plans a re generated and it will keep on building lineage graphs in DAG.So if one worker node goes down ideally we will not miss any data because the plan has already been generated and we can recover the data using the Lineage graph in DAG.\n",
    "* Ployglot--> Sparks supports multiple languages in Programming \n",
    "* Realtime\n",
    "* Speed\n",
    "* Rich Libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminologies \n",
    "* Driver and worker Process: These are JVM processes. Within one worker node ther could be multiple executors. Each executor runs its own JVM process.\n",
    "* Application: Its a single command or combination of multiple notebooks which is submitted to spark for execution.\n",
    "* Jobs: When an application is submitted to Spark driver, driver process converts the code in a job\n",
    "* Stages: Jobs are divided into stages. If the application demands shuffling rhe data across nodes, new stage is created. Number of stages are determined by numbers of shuffling operations. Join is an example of a **shuffling operation**\n",
    "* Tasks: Stages are divided further into multiple tasks. In a stage, all tasks would execute same logic. Each task will process one partition at a time. So the number of partitions in the distributed cluster will determine the number of tasks in each stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
